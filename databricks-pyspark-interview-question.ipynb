{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "936f3826-542a-412c-b87d-ed0e5c3c52d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Q-1 Create dataframe in pyspark databricks\n",
    "data = [('Alice','Badminton,Tenis'),('Bob','Tennis,Cricket'),('Julie','Cricket,Caram')]\n",
    "column=['Name','Hobbies']\n",
    "df = spark.createDataFrame(data,column)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8489fc2b-8100-459a-bcca-322792280cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-2 Use function split and explode\n",
    "data = [('Alice','Badminton,Tenis'),('Bob','Tennis,Cricket'),('Julie','Cricket,Caram')]\n",
    "column=['Name','Hobbies']\n",
    "df = spark.createDataFrame(data,column)\n",
    "display(df)\n",
    "\n",
    "# Perform Explode with split Operation\n",
    "from pyspark.sql.functions import split,explode\n",
    "df2 = df.select(df.Name,explode(split(df.Hobbies,',')).alias('Hobbie'))\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1d62cbe-714a-4b8a-8cd4-caec88804047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-3 Use of coalesce, with, when and otherwise function\n",
    "data = [('Goa','','AP'),('','AP',None),(None,'','bglr')]\n",
    "columns = ['City1','City2','City3']\n",
    "df = spark.createDataFrame(data,columns)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df1 = df.withColumn('firstnonNull',\n",
    "                    coalesce(when(df.City1==\"\",None).otherwise(df.City1),\n",
    "                             when(df.City2==\"\",None).otherwise(df.City2),\n",
    "                             when(df.City3=='',None).otherwise(df.City3)))\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1ee8b6-dcbc-4d8f-97f0-1d0f5f650900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Id</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>Steve</td></tr><tr><td>2</td><td>David</td></tr><tr><td>3</td><td>John</td></tr><tr><td>4</td><td>Shree</td></tr><tr><td>5</td><td>Helen</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Steve"
        ],
        [
         2,
         "David"
        ],
        [
         3,
         "John"
        ],
        [
         4,
         "Shree"
        ],
        [
         5,
         "Helen"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Id</th><th>Subject</th><th>Mark</th></tr></thead><tbody><tr><td>1</td><td>SQL</td><td>90</td></tr><tr><td>1</td><td>Pyspark</td><td>100</td></tr><tr><td>2</td><td>SQL</td><td>70</td></tr><tr><td>2</td><td>Pyspark</td><td>60</td></tr><tr><td>3</td><td>SQL</td><td>30</td></tr><tr><td>3</td><td>Pyspark</td><td>20</td></tr><tr><td>4</td><td>SQL</td><td>50</td></tr><tr><td>4</td><td>Pyspark</td><td>50</td></tr><tr><td>5</td><td>SQL</td><td>45</td></tr><tr><td>5</td><td>Pyspark</td><td>45</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "SQL",
         "90"
        ],
        [
         1,
         "Pyspark",
         "100"
        ],
        [
         2,
         "SQL",
         "70"
        ],
        [
         2,
         "Pyspark",
         "60"
        ],
        [
         3,
         "SQL",
         "30"
        ],
        [
         3,
         "Pyspark",
         "20"
        ],
        [
         4,
         "SQL",
         "50"
        ],
        [
         4,
         "Pyspark",
         "50"
        ],
        [
         5,
         "SQL",
         "45"
        ],
        [
         5,
         "Pyspark",
         "45"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q-4 Use of join in pyspark\n",
    "data1= [(1,\"Steve\"),(2,\"David\"),(3,\"John\"),(4,\"Shree\"),(5,\"Helen\")]\n",
    "data2=[(1,\"SQL\",\"90\"),(1,\"Pyspark\",100),(2,\"SQL\",70),(2,\"Pyspark\",60),(3,\"SQL\",30),(3,\"Pyspark\",20),(4,'SQL',50),(4,\"Pyspark\",50),(5,'SQL',45),(5,'Pyspark',45)]\n",
    "\n",
    "schema1=['Id','Name']\n",
    "schema2=['Id','Subject','Mark']\n",
    "\n",
    "df1=spark.createDataFrame(data1,schema1)\n",
    "df2=spark.createDataFrame(data2,schema2)\n",
    "\n",
    "display(df1)\n",
    "display(df2)\n",
    "\n",
    "# Applying join bwtween two dataframe\n",
    "%python\n",
    "df_join=df1.join(df2,df1.Id==df2.Id).drop(df1.Id)\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "316b24ba-bc0c-4d82-9f12-1ae4f068470c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Q-5 Use of group by, agg, sum function\n",
    "# Note:- dataframe refrenced from the above cell where we are creating the dataframe\n",
    "dfper = df_join.groupBy('Id','name').agg(sum('Mark')/count('*')).alias('Percentage')\n",
    "display(dfper)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84b09b7c-6b9a-4d5f-a888-0f3108e61c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-6 Use of dense_rank() function\n",
    "#Note to use function like rank() or row_num() just replace dense_rank() with rabk and row_num function,Other all logic is going tobe same.\n",
    "data = [(1,\"A\",1000,'IT'),(2,\"B\",1500,'IT'),(3,\"C\",2500,'IT'),(4,\"D\",3000,'HR'),(5,\"E\",2000,'HR'),(6,\"F\",1000,'HR'),(7,\"G\",4000,'Sales'),(8,\"H\",4000,'Sales'),(9,\"I\",1000,'Sales'),(10,\"J\",2000,'Sales')]\n",
    "\n",
    "scheme=[\"EmpId\",\"EmpName\",\"Salary\",\"DeptName\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=scheme)\n",
    "\n",
    "display(df)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "# Use of dense_rank() function\n",
    "df_rank = df.select('*',dense_rank().over(Window.partitionBy(df.DeptName).orderBy(df.Salary.desc()))).alias('desc_rank')\n",
    "display(df_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e6b9394-3c61-42d6-8e2e-8c3f2f7411df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-7 Use of filter operation\n",
    "# Note:- df_rank is created in the above cell, use that for refrence \n",
    "df_rank1 = df_rank.filter(df_rank.dense_rank==1)\n",
    "display(df_rank1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17019345-00b7-4b2a-9ea8-a189e75f9248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-7 Use of withColumn and to_date() function operation\n",
    "\n",
    "data1=[(100,\"Raj\",None,1,\"01-04-23\",50000),\n",
    "       (200,\"Joanne\",100,1,\"01-04-23\",4000),(200,\"Joanne\",100,1,\"13-04-23\",4500),(200,\"Joanne\",100,1,\"14-04-23\",4020)]\n",
    "schema1=[\"EmpId\",\"EmpName\",\"Mgrid\",\"deptid\",\"salarydt\",\"salary\"]\n",
    "df_salary=spark.createDataFrame(data1,schema1)\n",
    "display(df_salary)\n",
    "#department dataframe\n",
    "data2=[(1,\"IT\"),\n",
    "       (2,\"HR\")]\n",
    "schema2=[\"deptid\",\"deptname\"]\n",
    "df_dept=spark.createDataFrame(data2,schema2)\n",
    "display(df_dept)\n",
    "\n",
    "# Operations applied for the above dataframe\n",
    "from pyspark.sql.functions import *\n",
    "df=df_salary.withColumn('Newsaldt',to_date('salarydt','dd-MM-yy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bff14ea-a60f-4238-bf5a-ff716980bbea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-8 Way of reading file in Filestore with multiple read option operator.\n",
    "# File location and type\n",
    "\n",
    "file_location = \"/FileStore/tables/Churn_Modelling.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a96cdd-a5ce-4ac2-a74f-129971d8da94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-9 To get partation of dataframe in rdd\n",
    "# to check partation \n",
    "# Note:- dataframe used is refrence above\n",
    "df.rdd.getNumPartitions()\n",
    "display(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe98d802-aa5c-48a6-9fcf-578435117776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-9 To perform repartation \n",
    "# Note:- The Dataframe used is created above and refrenced here \n",
    "df=df.repartition(10)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0573da0a-f815-49fe-af13-9ff06ce5923c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-10 Schema Comparision in pyspark :- It is used when we need to store multiple dataframe into single dataframe and that dataframe needs to be stored in in some file.\n",
    "data1=[(1,\"Ram\",\"Male\",100),(2,\"Radhe\",\"Female\",200),(3,\"John\",\"Male\",250)]\n",
    "data2=[(101,\"John\",\"Male\",100),(102,\"Joanne\",\"Female\",250),(103,\"Smith\",\"Male\",250)]\n",
    "data3=[(1001,\"Maxwell\",\"IT\",200),(2,\"MSD\",\"HR\",350),(3,\"Virat\",\"IT\",300)]\n",
    "schema1=[\"Id\",\"Name\",\"Gender\",\"Salary\"]\n",
    "schema2=[\"Id\",\"Name\",\"Gender\",\"Salary\"]\n",
    "schema3=[\"Id\",\"Name\",\"DeptName\",\"Salary\"]\n",
    "df1=spark.createDataFrame(data1,schema1)\n",
    "df2=spark.createDataFrame(data2,schema2)\n",
    "df3=spark.createDataFrame(data3,schema3)\n",
    "display(df1)\n",
    "display(df2)\n",
    "display(df3)\n",
    "\n",
    "# Operation 1 : To check schema of two dataframe\n",
    "if df1.schema == df2.schema:\n",
    "    print(\"Schema Matched\")\n",
    "else:\n",
    "    print('Schema Not matched')\n",
    "\n",
    "# Operation 2 : To print schema of any dataframe\n",
    "print(df.schema)\n",
    "\n",
    "# Operation 3 : To subtrach column bw two dataframe\n",
    "print(list(set(df1.schema)-set(df3.schema)))\n",
    "print(list(set(df3.schema)-set(df1.schema)))\n",
    "\n",
    "# Operation 4 : Collect all the column\n",
    "all_Columns = df1.columns+df3.columns \n",
    "\n",
    "# Operation 5 : To get unique column\n",
    "unique_column=list(set(df1.columns+df3.columns)) \n",
    "\n",
    "# Operation 6 : To find missing column in a dataframe and use of lit operation \n",
    "from pyspark.sql.functions import *\n",
    "for i in unique_column:\n",
    "    if i not in df1.columns:\n",
    "        df1=df1.withColumn(i,lit(None))\n",
    "    if i not in df3.columns:\n",
    "        df3=df3.withColumn(i,lit(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a9e658f-d1e9-4fa0-a070-c26c38550930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-11 Use of StructFiled to enfore the schema\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data=[(1,('A-424','Noida','India')),(2,('M.15','Unnao','India'))]\n",
    "schema = StructType([\n",
    "     StructField('AddId', IntegerType(), True),\n",
    "     StructField('Address', StructType([\n",
    "         StructField('Add1', StringType(), True),\n",
    "         StructField('City', StringType(), True),\n",
    "         StructField('Country', StringType(), True)\n",
    "         ]))\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df619847-daab-41ec-b1ab-da3c24e65d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-12 Difference between subtract() and exceptall()\n",
    "#Subtract and Except All\n",
    "#subtract(): functions return a new dataframe with rows in the first dataframe that are not in second dataframe\n",
    "#exceptall():- functions also returns a new dataframe with rows in the first dataframe that are not in first dataframe but dones not remove dublicate.\n",
    "\n",
    "data = [(1,\"Mike\",\"2018\",\"10\",30000), \n",
    "    (2,\"John\",\"2010\",\"20\",40000), \n",
    "    (2,\"John\",\"2010\",\"20\",40000), \n",
    "    (3,\"Jack\",\"2010\",\"10\",40000), \n",
    "    (4,\"Charlee\",\"2005\",\"60\",35000), \n",
    "    (5,\"Guo\",\"2010\",\"40\",38000)]\n",
    "schema = [\"empid\",\"empname\",\"doj\", \"deptid\",\"salary\"]\n",
    "\n",
    "data1 = [(1,\"Mike\",\"2018\",\"10\",30000), \n",
    "           (4,\"Charlee\",\"2005\",\"60\",35000), \n",
    "           (5,\"Guo\",\"2010\",\"40\",38000)] \n",
    "schema1 = [\"empid\",\"empname\",\"doj\", \"deptid\",\"salary\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df1 = spark.createDataFrame(data1,schema1) \n",
    "display(df)\n",
    "display(df1)\n",
    "\n",
    "# Operation 1 exceptall() and subtract()\n",
    "display(df.exceptAll(df1))\n",
    "display(df.subtract(df1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08feec6a-5d95-41f9-9ad8-32e7dc2bcf02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-13 Different way of reading a file  PREMISSIVE,DROPMAILFORMED,FAILFAST\n",
    "\n",
    "# Premissive is by default when corrupted records comes up put that record into the column create to handle corrupt records.\n",
    "df1 = spark.read.option('header',true).option('mode','PREMISSIVE').schema(schema).csv(path='mention path')\n",
    "# DROPMALFORMED drop the records it self.\n",
    "df3 - spark.read.option('header',true).option('mode','DROPMALFORMED').schema(schema).csv(path='mention path')\n",
    "# FAILFAST throw error if data is not coorect.\n",
    "df2 = spark.read.option('header',true).option('mode','FAILFAST').schema(schema).csv(path='mention path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5590957-83ff-4e08-a097-eb97614a2f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-14 Date operation in pyspark\n",
    "from pyspark.sql.types import *\n",
    "data=[(1,'2024-01-01',\"I1\",10,1000),(2,\"2024-01-15\",\"I2\",20,2000),(3,\"2024-02-01\",\"I3\",10,1500),(4,\"2024-02-15\",\"I4\",20,2500),(5,\"2024-03-01\",\"I5\",30,3000),(6,\"2024-03-10\",\"I6\",40,3500),(7,\"2024-03-20\",\"I7\",20,2500),(8,\"2024-03-30\",\"I8\",10,1000)]\n",
    "schema=[\"SOId\",\"SODate\",\"ItemId\",\"ItemQty\",\"ItemValue\"]\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "display(df1)\n",
    "\n",
    "# Operation 1 cast() and DataType(), basically casting into datetype\n",
    "df1= df1.withColumn('SODate',df1.SODate.cast(DateType()))\n",
    "df1.printSchema()\n",
    "\n",
    "# Operation 2 Extract month and year from the date\n",
    "from pyspark.sql.functions import *\n",
    "df2=df1.select(month(df1.SODate).alias('Month'),year(df1.SODate).alias('Year'),df1.ItemValue)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "095caef3-aa86-4a44-be98-31432c2ca533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-15 Use of when and function\n",
    "from pyspark.sql.types import *\n",
    "data = [('IT','M'),('IT','F'),('IT','M'),('IT','M'),('HR','F'),('HR','F'),('HR','F'),('HR','F'),('HR','F'),('SALES','M'),('SALES','M'),('SALES','F'),('SALES','F'),('SALES','M'),('SALES','M'),('SALES','F'),]\n",
    "schema = ['DeptName','Gender']\n",
    "df1= spark.createDataFrame(data,schema)\n",
    "display(data)\n",
    "\n",
    "# Opertion 1\n",
    "from pyspark.sql.functions import *\n",
    "df2 = df1.select(df1.DeptName,when(df1.Gender=='M',1).alias('Male'),when(df1.Gender=='F',1).alias('Female'))\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2863c779-0d8d-491e-a933-55595ed0ba7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-16 Use of split Function\n",
    "data=[('Joanne',\"040-20215632\"),('Tom',\"044-23651023\"),('John',\"086-12456782\")]\n",
    "schema=[\"name\",\"phone\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "display(df)\n",
    "\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "# operation 1\n",
    "df=df.withColumn(\"std_code\",split(df.phone,'-').getItem(0))\n",
    "df=df.withColumn(\"phone_num\",split(df.phone,'-').getItem(1))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "597f845f-6161-4a6f-bc0a-e0f826a2c035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question 17 Use of union function \n",
    "simpleData1 = [(1,'Sagar','CSE','UP',88),(2,'Shivam','IT','MP',86),(3,'Munni','Mech','AP',78),]   \n",
    "column =[\"ID\",\"Student_Name\",\"Department_Name\",\"City\",\"Marks\"]\n",
    "df1 = spark.createDataFrame(simpleData1,column)\n",
    "\n",
    "simpleData2 = [(2,\"Raj\",\"CSE\",\"HP\"),(3,\"Raj\",\"IT\",\"HP\")]\n",
    "columns = [\"ID\",\"StudentName\",\"Departemnt\",\"City\"]\n",
    "df2 = spark.createDataFrame(simpleData2,columns)\n",
    "\n",
    "\n",
    "# Operation 1\n",
    "df1.union(df2)\n",
    "\n",
    "# Note :- As both table is having different set of colum its not possible to union both to fix the problem below code change is made and then union is applied.\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "df2=df2.withColumn(\"Marks\",lit(\"null\"))\n",
    "df3 = df1.union(df2)\n",
    "display(df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947f3356-2fbd-4803-8c7e-9ca49f9da751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-18 Use of regex operation rlike\n",
    "simpleData = ([1,\"Shivam\",\"UBFGD78945\"],[2,\"Sagar\",\"7338941808\"],[3,\"Muni\",\"9401330193\"])\n",
    "columns = [\"Id\",\"Name\",\"MobileNo\"]\n",
    "\n",
    "df1 = spark.createDataFrame(simpleData,columns) \n",
    "\n",
    "# Operation 1\n",
    "df2 = df1.select(\"*\").filter(col(\"MobileNo\").rlike('^[0-9]*$'))\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a0b0853-9223-401e-bf89-5039c849e309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-19 Use of pivot operation \n",
    "data = [(1, 'gaga', 'UK', '2022-04-26'), (2, 'baba', 'UK', '2022-01-10'), (3, 'mama', 'India', '2022-01-14'), (4, 'mama', 'USA', '2022-06-20'), (1, 'gaga', 'Canada', '2022-08-02'), (2, 'baba', 'Canada', '2022-07-08'), (3, 'zaza', 'India', '2022-03-28'), (4, 'mama', 'UK', '2022-04-30'), (1, 'gaga', 'USA', '2022-01-16'), (2, 'zaza', 'India', '2022-08-13'), (3, 'gaga', 'USA', '2022-09-06'), (4, 'mama', 'India', '2022-09-07'), (1, 'mama', 'Germany', '2022-10-05'), (2, 'baba', 'Germany', '2022-09-13'), (3, 'baba', 'Canada', '2022-09-11'), (4, 'zaza', 'UK', '2022-09-24'), (1, 'baba', 'Canada', '2022-09-03'), (2, 'baba', 'India', '2022-06-11'), (3, 'mama', 'UK', '2022-01-30'), (4, 'zaza', 'USA', '2022-12-15')]\n",
    "\n",
    "columns=['ID','Name','Country','Date_part']\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "\n",
    "# Operation - 1 \n",
    "from pyspark.sql.functions import *\n",
    "df1 = df.groupBy('ID','Date_part').pivot('Name').agg(first('Country'))\n",
    "display(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "890c5c91-eb4e-41da-9857-3459a98213cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-20 Use of row_num() and Windows funcition\n",
    "# Note:- The above cell dataframe is refrenced here\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "# Operation 1\n",
    "windowspec = Window.orderBy(col('Id'))\n",
    "df = df.withColumn('rownumber',row_number().over(windowspec))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd37c1ca-2656-4253-8ee4-cbc9c889e6d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-21 How to find occurace of duplicate row with the help of primary key\n",
    "data = [(1,'Anish',30000,'enganish213@outlook.com'),(1,'Anish',30000,'enganish213@outlook.com'),(1,'Anish',30000,'enganish213@outlook.com'),(2,'Bhavesh',40000,'bhavesh213@gmail.com'),(2,'Bhavesh',40000,'bhavesh213@gmail.com'),(3,'Chandan',50000,'chandan213@yahoo.com'),(3,'Chandan',50000,'chandan213@yahoo.com'),(4,'Dinesh',60000,'dinesh213@yahoo.com'),(4,'Dinesh',60000,'dinesh213@yahoo.com'),(5,'Esha',70000,'rakesh213@yahoo.com')]\n",
    "columns = ['Id','Name','Salary','Email']\n",
    "df = spark.createDataFrame(data,columns)\n",
    "\n",
    "# Operation 1\n",
    "dublicate_email = (\n",
    "    df.groupBy('Email').agg(count('Email').alias('EmailCount')).filter(col('EmailCount') >1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f88bb7b7-8de1-408b-8840-c66202ccc71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q-22 Use of Windows Paration By and order function \n",
    "from pyspark.sql.functions import col, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [\n",
    "    (1, \"HR\", 5000),\n",
    "    (2, \"HR\", 6000),\n",
    "    (3, \"IT\", 7000),\n",
    "    (4, \"IT\", 8000),\n",
    "    (5, \"HR\", 7000),\n",
    "    (6, \"IT\", 6000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"employee_id\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Operation 1 Apply Order by and partationBy\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Operation 2 Apply rank function to rank employees within each department based on salary\n",
    "df_with_rank = df.withColumn(\"rank\", rank().over(windowSpec))\n",
    "\n",
    "# Show the result\n",
    "df_with_rank.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}